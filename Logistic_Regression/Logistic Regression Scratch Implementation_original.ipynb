{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv(\"amazon_baby_subset.csv\")\n",
    "products = products.astype(str)\n",
    "#pd.read_csv intelligently converts input to python datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n",
       "      <td>We wanted to get something to keep track of ou...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n",
       "      <td>My daughter had her 1st baby over a year ago. ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lamaze Peekaboo, I Love You</td>\n",
       "      <td>One of baby's first and favorite books, and it...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SoftPlay Peek-A-Boo Where's Elmo A Children's ...</td>\n",
       "      <td>Very cute interactive book! My son loves this ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Our Baby Girl Memory Book</td>\n",
       "      <td>Beautiful book, I love it to record cherished ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hunnt&amp;reg; Falling Flowers and Birds Kids Nurs...</td>\n",
       "      <td>Try this out for a spring project !Easy ,fun a...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Blessed By Pope Benedict XVI Divine Mercy Full...</td>\n",
       "      <td>very nice Divine Mercy Pendant of Jesus now on...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cloth Diaper Pins Stainless Steel Traditional ...</td>\n",
       "      <td>We bought the pins as my 6 year old Autistic s...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cloth Diaper Pins Stainless Steel Traditional ...</td>\n",
       "      <td>It has been many years since we needed diaper ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "1    Nature's Lullabies Second Year Sticker Calendar   \n",
       "2    Nature's Lullabies Second Year Sticker Calendar   \n",
       "3                        Lamaze Peekaboo, I Love You   \n",
       "4  SoftPlay Peek-A-Boo Where's Elmo A Children's ...   \n",
       "5                          Our Baby Girl Memory Book   \n",
       "6  Hunnt&reg; Falling Flowers and Birds Kids Nurs...   \n",
       "7  Blessed By Pope Benedict XVI Divine Mercy Full...   \n",
       "8  Cloth Diaper Pins Stainless Steel Traditional ...   \n",
       "9  Cloth Diaper Pins Stainless Steel Traditional ...   \n",
       "\n",
       "                                              review rating sentiment  \n",
       "1  We wanted to get something to keep track of ou...      5         1  \n",
       "2  My daughter had her 1st baby over a year ago. ...      5         1  \n",
       "3  One of baby's first and favorite books, and it...      4         1  \n",
       "4  Very cute interactive book! My son loves this ...      5         1  \n",
       "5  Beautiful book, I love it to record cherished ...      5         1  \n",
       "6  Try this out for a spring project !Easy ,fun a...      5         1  \n",
       "7  very nice Divine Mercy Pendant of Jesus now on...      5         1  \n",
       "8  We bought the pins as my 6 year old Autistic s...      4         1  \n",
       "9  It has been many years since we needed diaper ...      5         1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "products['rating'] = products['rating'].astype(int)\n",
    "products['sentiment'] = products['sentiment'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products = products.fillna({'reveiw':''}) #fill in N/A's in the review column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write a function remove_punctuation that takes a line of text and removes all punctuation from that text\n",
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    return text.translate(None, string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products['review_clean'] = products['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read \"important_words.json\" file\n",
    "import json\n",
    "\n",
    "with open('important_words.json') as data_file:\n",
    "    important_words = json.load(data_file)\n",
    "with open('module-4-assignment-train-idx.json') as data_file:\n",
    "    train_index = json.load(data_file)\n",
    "with open('module-4-assignment-validation-idx.json') as data_file:\n",
    "    validation_index = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we proceed with the second item. For each word in important_words, \n",
    "#we compute a count for the number of times the word occurs in the review.\n",
    "for word in important_words:\n",
    "    products[word] = products['review_clean'].apply(lambda s: s.split().count(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53072, 198)\n"
     ]
    }
   ],
   "source": [
    "print products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = products.iloc[train_index]\n",
    "validation_data = products.iloc[validation_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42361, 199)\n",
      "(10711, 199)\n"
     ]
    }
   ],
   "source": [
    "print train_data.shape\n",
    "print validation_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_numpy_data(dataframe, features, label):\n",
    "    dataframe['constant'] = 1\n",
    "    features = ['constant'] + features\n",
    "    features_frame = dataframe[features]\n",
    "    features_matrix = features_frame.as_matrix()\n",
    "    label_sarray = dataframe[label]\n",
    "    label_array = label_sarray.as_matrix()\n",
    "    return(features_matrix, label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chnagikchoi/anaconda/envs/dato-env/lib/python2.7/site-packages/IPython/kernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from IPython.kernel.zmq import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "feature_matrix_train, sentiment_train = get_numpy_data(train_data, important_words, 'sentiment')\n",
    "feature_matrix_valid, sentiment_valid = get_numpy_data(validation_data, important_words, 'sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Building a logistic regression\n",
    "We gonna use a same functions from logistic regression without l2_penalty\n",
    "and add l2_penalty on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Estimating conditional probability with link function\n",
    "#produces probabilistic estimate for P(y_i = +1| x_i, w).\n",
    "#estimate ranges between 0 and 1\n",
    "def predict_probability(feature_matrix, coefficients):\n",
    "    #Take dot product of feature_matrix and coefficients\n",
    "    score = np.dot(feature_matrix, coefficients)\n",
    "    #Compute P(y_i = +1|x_i, w) using the link function\n",
    "    predictions = 1/(1+np.exp(-score))\n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute derivative of log likelihood with respect to a single coefficient\n",
    "def feature_derivative_with_L2(errors, feature, coefficient, l2_penalty, feature_is_constant):\n",
    "    #Compute the dot product of errors and feature(without L2 penalty)\n",
    "    derivative = np.dot(errors, feature)\n",
    "    \n",
    "    #add L2 penalty term for any feature that isn't the intercept\n",
    "    if not feature_is_constant:\n",
    "        derivative = derivative - (2*l2_penalty*coefficient)\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty):\n",
    "    indicator = (sentiment == +1)\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    lp = np.sum((indicator-1) * scores - np.log(1. + np.exp(-scores))) - l2_penalty*np.sum(coefficients[1:]**2)\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, step_size, l2_penalty, max_iter):\n",
    "    coefficients = np.array(initial_coefficients) #make sure it's a numpy array\n",
    "    for itr in xrange(max_iter):\n",
    "        #Predict P(y_i = +1|x_1,w) using your predict_probability() function\n",
    "        predictions = predict_probability(feature_matrix, coefficients)\n",
    "        \n",
    "        #compute indicator value for (y_i = +1)\n",
    "        indicator = (sentiment == +1)\n",
    "        \n",
    "        #Compute the errors as indicator - predictions\n",
    "        errors = indicator - predictions\n",
    "        \n",
    "        for j in xrange(len(coefficients)): #loop over each coefficient\n",
    "            is_intercept = (j==0)\n",
    "            #Recall that feature_matrix[:,j] is the feature column associated with coefficients[j]\n",
    "            #compute the derivative for coefficients[j]. Save it in a variable called derivative\n",
    "            derivative = feature_derivative_with_L2(errors, feature_matrix[:,j], coefficients[j], l2_penalty, is_intercept)\n",
    "            #add step size times the derivative to the current coefficient(l2_penalty is already added)\n",
    "            coefficients[j] = coefficients[j] + (step_size * derivative)\n",
    "        \n",
    "        #Checking whether log likelihood is increasing\n",
    "        if itr <= 15 or (itr <= 100 and itr %10 ==0) or \\\n",
    "            (itr <= 1000 and itr %100 ==0) or (itr <= 10000 and itr %1000 ==0) or itr % 10000 ==0:\n",
    "                lp = compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty)\n",
    "                print 'iteration %*d : log likelihood of observed labels = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter ))), itr, lp)\n",
    "    return coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_matrix = feature_matrix_train\n",
    "sentiment = sentiment_train\n",
    "initial_coefficients = np.zeros(194)\n",
    "step_size = 5e-6\n",
    "max_iter = 501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0 : log likelihood of observed labels = -29179.39138303\n",
      "iteration   1 : log likelihood of observed labels = -29003.71259047\n",
      "iteration   2 : log likelihood of observed labels = -28834.66187288\n",
      "iteration   3 : log likelihood of observed labels = -28671.70781507\n",
      "iteration   4 : log likelihood of observed labels = -28514.43078198\n",
      "iteration   5 : log likelihood of observed labels = -28362.48344665\n",
      "iteration   6 : log likelihood of observed labels = -28215.56713122\n",
      "iteration   7 : log likelihood of observed labels = -28073.41743783\n",
      "iteration   8 : log likelihood of observed labels = -27935.79536396\n",
      "iteration   9 : log likelihood of observed labels = -27802.48168669\n",
      "iteration  10 : log likelihood of observed labels = -27673.27331484\n",
      "iteration  11 : log likelihood of observed labels = -27547.98083656\n",
      "iteration  12 : log likelihood of observed labels = -27426.42679977\n",
      "iteration  13 : log likelihood of observed labels = -27308.44444728\n",
      "iteration  14 : log likelihood of observed labels = -27193.87673876\n",
      "iteration  15 : log likelihood of observed labels = -27082.57555831\n",
      "iteration  20 : log likelihood of observed labels = -26570.43059938\n",
      "iteration  30 : log likelihood of observed labels = -25725.48742389\n",
      "iteration  40 : log likelihood of observed labels = -25055.53326910\n",
      "iteration  50 : log likelihood of observed labels = -24509.63590026\n",
      "iteration  60 : log likelihood of observed labels = -24054.97906083\n",
      "iteration  70 : log likelihood of observed labels = -23669.51640848\n",
      "iteration  80 : log likelihood of observed labels = -23337.89167628\n",
      "iteration  90 : log likelihood of observed labels = -23049.07066021\n",
      "iteration 100 : log likelihood of observed labels = -22794.90974921\n",
      "iteration 200 : log likelihood of observed labels = -21283.29527353\n",
      "iteration 300 : log likelihood of observed labels = -20570.97485473\n",
      "iteration 400 : log likelihood of observed labels = -20152.21466944\n",
      "iteration 500 : log likelihood of observed labels = -19876.62333410\n"
     ]
    }
   ],
   "source": [
    "l2_penalty = 0\n",
    "coefficients_0_penalty = logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, step_size, l2_penalty, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0 : log likelihood of observed labels = -29179.39508175\n",
      "iteration   1 : log likelihood of observed labels = -29003.73417180\n",
      "iteration   2 : log likelihood of observed labels = -28834.71441858\n",
      "iteration   3 : log likelihood of observed labels = -28671.80345068\n",
      "iteration   4 : log likelihood of observed labels = -28514.58077957\n",
      "iteration   5 : log likelihood of observed labels = -28362.69830317\n",
      "iteration   6 : log likelihood of observed labels = -28215.85663259\n",
      "iteration   7 : log likelihood of observed labels = -28073.79071393\n",
      "iteration   8 : log likelihood of observed labels = -27936.26093762\n",
      "iteration   9 : log likelihood of observed labels = -27803.04751805\n",
      "iteration  10 : log likelihood of observed labels = -27673.94684207\n",
      "iteration  11 : log likelihood of observed labels = -27548.76901327\n",
      "iteration  12 : log likelihood of observed labels = -27427.33612958\n",
      "iteration  13 : log likelihood of observed labels = -27309.48101569\n",
      "iteration  14 : log likelihood of observed labels = -27195.04624253\n",
      "iteration  15 : log likelihood of observed labels = -27083.88333261\n",
      "iteration  20 : log likelihood of observed labels = -26572.49874392\n",
      "iteration  30 : log likelihood of observed labels = -25729.32604153\n",
      "iteration  40 : log likelihood of observed labels = -25061.34245801\n",
      "iteration  50 : log likelihood of observed labels = -24517.52091982\n",
      "iteration  60 : log likelihood of observed labels = -24064.99093939\n",
      "iteration  70 : log likelihood of observed labels = -23681.67373669\n",
      "iteration  80 : log likelihood of observed labels = -23352.19298741\n",
      "iteration  90 : log likelihood of observed labels = -23065.50180166\n",
      "iteration 100 : log likelihood of observed labels = -22813.44844580\n",
      "iteration 200 : log likelihood of observed labels = -21321.14164794\n",
      "iteration 300 : log likelihood of observed labels = -20624.98634439\n",
      "iteration 400 : log likelihood of observed labels = -20219.92048845\n",
      "iteration 500 : log likelihood of observed labels = -19956.11341777\n"
     ]
    }
   ],
   "source": [
    "l2_penalty = 4\n",
    "coefficients_4_penalty = logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, step_size, l2_penalty, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0 : log likelihood of observed labels = -29179.40062984\n",
      "iteration   1 : log likelihood of observed labels = -29003.76654163\n",
      "iteration   2 : log likelihood of observed labels = -28834.79322654\n",
      "iteration   3 : log likelihood of observed labels = -28671.94687528\n",
      "iteration   4 : log likelihood of observed labels = -28514.80571589\n",
      "iteration   5 : log likelihood of observed labels = -28363.02048079\n",
      "iteration   6 : log likelihood of observed labels = -28216.29071186\n",
      "iteration   7 : log likelihood of observed labels = -28074.35036891\n",
      "iteration   8 : log likelihood of observed labels = -27936.95892966\n",
      "iteration   9 : log likelihood of observed labels = -27803.89576265\n",
      "iteration  10 : log likelihood of observed labels = -27674.95647005\n",
      "iteration  11 : log likelihood of observed labels = -27549.95042714\n",
      "iteration  12 : log likelihood of observed labels = -27428.69905549\n",
      "iteration  13 : log likelihood of observed labels = -27311.03455140\n",
      "iteration  14 : log likelihood of observed labels = -27196.79890162\n",
      "iteration  15 : log likelihood of observed labels = -27085.84308528\n",
      "iteration  20 : log likelihood of observed labels = -26575.59697506\n",
      "iteration  30 : log likelihood of observed labels = -25735.07304608\n",
      "iteration  40 : log likelihood of observed labels = -25070.03447306\n",
      "iteration  50 : log likelihood of observed labels = -24529.31188025\n",
      "iteration  60 : log likelihood of observed labels = -24079.95349572\n",
      "iteration  70 : log likelihood of observed labels = -23699.83199186\n",
      "iteration  80 : log likelihood of observed labels = -23373.54108747\n",
      "iteration  90 : log likelihood of observed labels = -23090.01500055\n",
      "iteration 100 : log likelihood of observed labels = -22841.08995135\n",
      "iteration 200 : log likelihood of observed labels = -21377.25595328\n",
      "iteration 300 : log likelihood of observed labels = -20704.63995428\n",
      "iteration 400 : log likelihood of observed labels = -20319.25685307\n",
      "iteration 500 : log likelihood of observed labels = -20072.16321721\n"
     ]
    }
   ],
   "source": [
    "l2_penalty = 10\n",
    "coefficients_10_penalty = logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, step_size, l2_penalty, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0 : log likelihood of observed labels = -29179.48385120\n",
      "iteration   1 : log likelihood of observed labels = -29004.25177457\n",
      "iteration   2 : log likelihood of observed labels = -28835.97382190\n",
      "iteration   3 : log likelihood of observed labels = -28674.09410083\n",
      "iteration   4 : log likelihood of observed labels = -28518.17112932\n",
      "iteration   5 : log likelihood of observed labels = -28367.83774654\n",
      "iteration   6 : log likelihood of observed labels = -28222.77708939\n",
      "iteration   7 : log likelihood of observed labels = -28082.70799392\n",
      "iteration   8 : log likelihood of observed labels = -27947.37595368\n",
      "iteration   9 : log likelihood of observed labels = -27816.54738615\n",
      "iteration  10 : log likelihood of observed labels = -27690.00588850\n",
      "iteration  11 : log likelihood of observed labels = -27567.54970126\n",
      "iteration  12 : log likelihood of observed labels = -27448.98991327\n",
      "iteration  13 : log likelihood of observed labels = -27334.14912742\n",
      "iteration  14 : log likelihood of observed labels = -27222.86041863\n",
      "iteration  15 : log likelihood of observed labels = -27114.96648229\n",
      "iteration  20 : log likelihood of observed labels = -26621.50201299\n",
      "iteration  30 : log likelihood of observed labels = -25819.72803950\n",
      "iteration  40 : log likelihood of observed labels = -25197.34035501\n",
      "iteration  50 : log likelihood of observed labels = -24701.03698195\n",
      "iteration  60 : log likelihood of observed labels = -24296.66378580\n",
      "iteration  70 : log likelihood of observed labels = -23961.38842316\n",
      "iteration  80 : log likelihood of observed labels = -23679.38088853\n",
      "iteration  90 : log likelihood of observed labels = -23439.31824267\n",
      "iteration 100 : log likelihood of observed labels = -23232.88192018\n",
      "iteration 200 : log likelihood of observed labels = -22133.50726528\n",
      "iteration 300 : log likelihood of observed labels = -21730.03957488\n",
      "iteration 400 : log likelihood of observed labels = -21545.87572145\n",
      "iteration 500 : log likelihood of observed labels = -21451.95551390\n"
     ]
    }
   ],
   "source": [
    "l2_penalty = 1e2\n",
    "coefficients_1e2_penalty = logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, step_size, l2_penalty, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0 : log likelihood of observed labels = -29180.31606471\n",
      "iteration   1 : log likelihood of observed labels = -29009.07176112\n",
      "iteration   2 : log likelihood of observed labels = -28847.62378912\n",
      "iteration   3 : log likelihood of observed labels = -28695.14439397\n",
      "iteration   4 : log likelihood of observed labels = -28550.95060743\n",
      "iteration   5 : log likelihood of observed labels = -28414.45771129\n",
      "iteration   6 : log likelihood of observed labels = -28285.15124375\n",
      "iteration   7 : log likelihood of observed labels = -28162.56976044\n",
      "iteration   8 : log likelihood of observed labels = -28046.29387744\n",
      "iteration   9 : log likelihood of observed labels = -27935.93902900\n",
      "iteration  10 : log likelihood of observed labels = -27831.15045502\n",
      "iteration  11 : log likelihood of observed labels = -27731.59955260\n",
      "iteration  12 : log likelihood of observed labels = -27636.98108219\n",
      "iteration  13 : log likelihood of observed labels = -27547.01092670\n",
      "iteration  14 : log likelihood of observed labels = -27461.42422295\n",
      "iteration  15 : log likelihood of observed labels = -27379.97375625\n",
      "iteration  20 : log likelihood of observed labels = -27027.18208317\n",
      "iteration  30 : log likelihood of observed labels = -26527.22737267\n",
      "iteration  40 : log likelihood of observed labels = -26206.59048765\n",
      "iteration  50 : log likelihood of observed labels = -25995.96903148\n",
      "iteration  60 : log likelihood of observed labels = -25854.95710284\n",
      "iteration  70 : log likelihood of observed labels = -25759.08109950\n",
      "iteration  80 : log likelihood of observed labels = -25693.05688014\n",
      "iteration  90 : log likelihood of observed labels = -25647.09929349\n",
      "iteration 100 : log likelihood of observed labels = -25614.81468705\n",
      "iteration 200 : log likelihood of observed labels = -25536.20998919\n",
      "iteration 300 : log likelihood of observed labels = -25532.57691220\n",
      "iteration 400 : log likelihood of observed labels = -25532.35543765\n",
      "iteration 500 : log likelihood of observed labels = -25532.33970049\n"
     ]
    }
   ],
   "source": [
    "l2_penalty = 1e3\n",
    "coefficients_1e3_penalty = logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, step_size, l2_penalty, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0 : log likelihood of observed labels = -29271.85955115\n",
      "iteration   1 : log likelihood of observed labels = -29271.71006589\n",
      "iteration   2 : log likelihood of observed labels = -29271.65738833\n",
      "iteration   3 : log likelihood of observed labels = -29271.61189923\n",
      "iteration   4 : log likelihood of observed labels = -29271.57079975\n",
      "iteration   5 : log likelihood of observed labels = -29271.53358505\n",
      "iteration   6 : log likelihood of observed labels = -29271.49988440\n",
      "iteration   7 : log likelihood of observed labels = -29271.46936584\n",
      "iteration   8 : log likelihood of observed labels = -29271.44172890\n",
      "iteration   9 : log likelihood of observed labels = -29271.41670149\n",
      "iteration  10 : log likelihood of observed labels = -29271.39403722\n",
      "iteration  11 : log likelihood of observed labels = -29271.37351294\n",
      "iteration  12 : log likelihood of observed labels = -29271.35492661\n",
      "iteration  13 : log likelihood of observed labels = -29271.33809523\n",
      "iteration  14 : log likelihood of observed labels = -29271.32285309\n",
      "iteration  15 : log likelihood of observed labels = -29271.30905015\n",
      "iteration  20 : log likelihood of observed labels = -29271.25729150\n",
      "iteration  30 : log likelihood of observed labels = -29271.20657205\n",
      "iteration  40 : log likelihood of observed labels = -29271.18775997\n",
      "iteration  50 : log likelihood of observed labels = -29271.18078247\n",
      "iteration  60 : log likelihood of observed labels = -29271.17819447\n",
      "iteration  70 : log likelihood of observed labels = -29271.17723457\n",
      "iteration  80 : log likelihood of observed labels = -29271.17687853\n",
      "iteration  90 : log likelihood of observed labels = -29271.17674648\n",
      "iteration 100 : log likelihood of observed labels = -29271.17669750\n",
      "iteration 200 : log likelihood of observed labels = -29271.17666862\n",
      "iteration 300 : log likelihood of observed labels = -29271.17666862\n",
      "iteration 400 : log likelihood of observed labels = -29271.17666862\n",
      "iteration 500 : log likelihood of observed labels = -29271.17666862\n"
     ]
    }
   ],
   "source": [
    "l2_penalty = 1e5\n",
    "coefficients_1e5_penalty = logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, step_size, l2_penalty, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#How to build table in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from astropy.table import Table, Columnfrom astropy.table import Table, Column\n",
    "#table= Table()\n",
    "#table['word'] = important_words\n",
    "#table['l2_penalty_0'] = coefficients_0_penalty[1:]\n",
    "#table['l2_penalty_4'] = coefficients_4_penalty[1:]\n",
    "#table['l2_penalty_10'] = coefficients_10_penalty[1:]\n",
    "#table['l2_penalty_1e2'] = coefficients_1e2_penalty[1:]\n",
    "#table['l2_penalty_1e5'] = coefficients_1e5_penalty[1:]\n",
    "#print table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   l2_penalty_0  l2_penalty_10  l2_penalty_1e2  l2_penalty_1e5  l2_penalty_4  \\\n",
      "1      0.012753       0.012115        0.007247       -0.001827      0.012495   \n",
      "2      0.801625       0.789935        0.701425        0.008950      0.796897   \n",
      "3      1.058554       1.039529        0.896644        0.009042      1.050856   \n",
      "4     -0.000104       0.000556        0.005481        0.000418      0.000163   \n",
      "5     -0.287021      -0.284564       -0.265993       -0.008127     -0.286027   \n",
      "6     -0.003384      -0.003527       -0.004635       -0.000827     -0.003442   \n",
      "7      0.984559       0.967362        0.838245        0.008808      0.977600   \n",
      "8      0.524419       0.516917        0.460235        0.005941      0.521385   \n",
      "9     -0.086968      -0.084883       -0.069109        0.000611     -0.086125   \n",
      "\n",
      "     word  \n",
      "1     one  \n",
      "2   great  \n",
      "3    love  \n",
      "4     use  \n",
      "5   would  \n",
      "6    like  \n",
      "7    easy  \n",
      "8  little  \n",
      "9    seat  \n",
      "(193, 6)\n"
     ]
    }
   ],
   "source": [
    "#but we gonna use this DataFrame\n",
    "import pandas as pd\n",
    "table = pd.DataFrame({'word': important_words, 'l2_penalty_0': coefficients_0_penalty[1:],\n",
    "                  'l2_penalty_4': coefficients_4_penalty[1:], 'l2_penalty_10': coefficients_10_penalty[1:],\n",
    "                  'l2_penalty_1e2': coefficients_1e2_penalty[1:], 'l2_penalty_1e5': coefficients_1e5_penalty[1:]})\n",
    "print table[1:10]\n",
    "print table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22      loves\n",
      "7        easy\n",
      "33    perfect\n",
      "2       great\n",
      "82      happy\n",
      "Name: word, dtype: object\n",
      "171       broke\n",
      "168    returned\n",
      "112       waste\n",
      "113      return\n",
      "96        money\n",
      "Name: word, dtype: object\n"
     ]
    }
   ],
   "source": [
    "table = table.sort(['l2_penalty_0'], ascending=[0])\n",
    "positive_words = table[1:6]['word']\n",
    "negative_words = table[-6:-1]['word']\n",
    "print positive_words\n",
    "print negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-30b83c7c1ef7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mmake_coefficient_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_penalty_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-93-30b83c7c1ef7>\u001b[0m in \u001b[0;36mmake_coefficient_plot\u001b[0;34m(table, positive_words, negative_words, l2_penalty_list)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmap_positive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         plt.plot(xx, table_positive_words[i:i+1].as_matrix().flatten(),\n\u001b[0;32m---> 20\u001b[0;31m                  '-', label=positive_words[i], linewidth=4.0, color=color)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chnagikchoi/anaconda/envs/dato-env/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chnagikchoi/anaconda/envs/dato-env/lib/python2.7/site-packages/pandas/core/index.pyc\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'boolean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_value (pandas/index.c:3113)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_value (pandas/index.c:2844)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3704)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:7224)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:7162)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm8AAAFwCAYAAADuXpxbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1095f22d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "\n",
    "def make_coefficient_plot(table, positive_words, negative_words, l2_penalty_list):\n",
    "    cmap_positive = plt.get_cmap('Reds')\n",
    "    cmap_negative = plt.get_cmap('Blues')\n",
    "    \n",
    "    xx = l2_penalty_list\n",
    "    plt.plot(xx, [0.]*len(xx), '--', lw=1, color='k')\n",
    "    \n",
    "    table_positive_words = table[table['word'].isin(positive_words)]\n",
    "    table_negative_words = table[table['word'].isin(negative_words)]\n",
    "    del table_positive_words['word']\n",
    "    del table_negative_words['word']\n",
    "    \n",
    "    for i in xrange(len(positive_words)):\n",
    "        color = cmap_positive(0.8*((i+1)/(len(positive_words)*1.2)+0.15))\n",
    "        plt.plot(xx, table_positive_words[i:i+1].as_matrix().flatten(),\n",
    "                 '-', label=positive_words[i], linewidth=4.0, color=color)\n",
    "        \n",
    "    for i in xrange(len(negative_words)):\n",
    "        color = cmap_negative(0.8*((i+1)/(len(negative_words)*1.2)+0.15))\n",
    "        plt.plot(xx, table_negative_words[i:i+1].as_matrix().flatten(),\n",
    "                 '-', label=negative_words[i], linewidth=4.0, color=color)\n",
    "        \n",
    "    plt.legend(loc='best', ncol=3, prop={'size':16}, columnspacing=0.5)\n",
    "    plt.axis([1, 1e5, -1, 2])\n",
    "    plt.title('Coefficient path')\n",
    "    plt.xlabel('L2 penalty ($\\lambda$)')\n",
    "    plt.ylabel('Coefficient value')\n",
    "    plt.xscale('log')\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "make_coefficient_plot(table, positive_words, negative_words, l2_penalty_list=[0, 4, 10, 1e2, 1e3, 1e5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'table_positive_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ea4b9c35d5e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtable_positive_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'table_positive_words' is not defined"
     ]
    }
   ],
   "source": [
    " table_positive_words = table[table['word'].isin(positive_words)]\n",
    "table_positive_words[1:2].as_matrix().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(feature_matrix, coefficients, sentiment):\n",
    "    #compute scores using feature_matrix, coefficients\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    #threshold scores by 0\n",
    "    positive = scores > 0\n",
    "    negative = scores <= 0\n",
    "    scores[positive] = 1\n",
    "    scores[negative] = -1\n",
    "\n",
    "    correct = float((scores == sentiment).sum())\n",
    "    total = float(len(sentiment))\n",
    "    accuracy = float(correct / total)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_0_penalty = get_accuracy(feature_matrix_train, coefficients_0_penalty, sentiment_train)\n",
    "accuracy_4_penalty = get_accuracy(feature_matrix_train, coefficients_4_penalty, sentiment_train)\n",
    "accuracy_10_penalty = get_accuracy(feature_matrix_train, coefficients_10_penalty, sentiment_train)\n",
    "accuracy_1e2_penalty = get_accuracy(feature_matrix_train, coefficients_1e2_penalty, sentiment_train)\n",
    "accuracy_1e3_penalty = get_accuracy(feature_matrix_train, coefficients_1e3_penalty, sentiment_train)\n",
    "accuracy_1e5_penalty = get_accuracy(feature_matrix_train, coefficients_1e5_penalty, sentiment_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.785156157787\n",
      "0.785108944548\n",
      "0.784990911452\n",
      "0.783975826822\n",
      "0.775855149784\n",
      "0.680366374731\n"
     ]
    }
   ],
   "source": [
    "print accuracy_0_penalty\n",
    "print accuracy_4_penalty\n",
    "print accuracy_10_penalty\n",
    "print accuracy_1e2_penalty\n",
    "print accuracy_1e3_penalty\n",
    "print accuracy_1e5_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_0_penalty_valid = get_accuracy(feature_matrix_valid, coefficients_0_penalty, sentiment_valid)\n",
    "accuracy_4_penalty_valid = get_accuracy(feature_matrix_valid, coefficients_4_penalty, sentiment_valid)\n",
    "accuracy_10_penalty_valid = get_accuracy(feature_matrix_valid, coefficients_10_penalty, sentiment_valid)\n",
    "accuracy_1e2_penalty_valid = get_accuracy(feature_matrix_valid, coefficients_1e2_penalty, sentiment_valid)\n",
    "accuracy_1e3_penalty_valid = get_accuracy(feature_matrix_valid, coefficients_1e3_penalty, sentiment_valid)\n",
    "accuracy_1e5_penalty_valid = get_accuracy(feature_matrix_valid, coefficients_1e5_penalty, sentiment_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78143964149\n",
      "0.781252917561\n",
      "0.781533003454\n",
      "0.782653347026\n",
      "0.780225935954\n",
      "0.74848286808\n"
     ]
    }
   ],
   "source": [
    "print accuracy_0_penalty_valid\n",
    "print accuracy_4_penalty_valid\n",
    "print accuracy_10_penalty_valid\n",
    "print accuracy_1e2_penalty_valid\n",
    "print accuracy_1e3_penalty_valid\n",
    "print accuracy_1e5_penalty_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
